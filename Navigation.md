# ğŸ“ ADF â€” UI NavigationğŸ’¡

A focused, stepâ€‘byâ€‘step **UI walkthrough** (click-by-click). Use this as a quick reference while you work inside the Azure Portal and ADF Studio.

---

## ğŸ¯ 1. Azure Portal â†’ Resource Group â†’ Storage Account

1. Open **Azure Portal** (portal.azure.com).
2. Left menu â†’ **Resource groups** â†’ click your resource group (example: `myRG-adf`).
3. Inside the RG list, find and click your **Storage account** (example: `mystorageacct123`).

ğŸ” **Inside the Storage Account**

* Left menu â†’ **Data storage** â†’ click **Containers**.
* Click the container you want (example: `pracccontainer`).
* To upload a file: inside the container click **Upload** (top menu) â†’ browse local file â†’ **Upload**.
* To create a new container: **+ Container** â†’ enter name (e.g. `outputcontainer`) â†’ **Create**.
* To create folders inside a container: open container â†’ click **+ Folder** â†’ name folder (e.g. `raw/incoming`).

ğŸ’¡ **Tips:**

* Use container names that reflect lifecycle: `raw`, `staging`, `processed`, `archive`.
* If you donâ€™t see **Containers**, expand the **Data storage** section (it may be collapsed).

---

## ğŸ¯ 2. Azure Portal â†’ Resource Group â†’ Data Factory â†’ Open ADF Studio

1. In the Resource Group view, click your **Data Factory** resource (example: `myADFWorkshopFactory`).
2. Top center: click **Author & Monitor** (this opens ADF Studio in a new tab).

ğŸ’¡ **Tip:** If the portal shows a button labelled **Open** or **Author & Monitor**, that will also open the Studio.

---

## ğŸ¯ 3. ADF Studio â€” Main Areas & How to Open Them

When ADF Studio opens youâ€™ll see a left-side vertical menu and a top bar. Primary sections:

* **Home** (landing page) â€” quick links and recent items.
* **Author** (pencil icon) â€” create pipelines, datasets, data flows, linked services.
* **Monitor** (graph icon) â€” view pipeline & activity runs, re-run, check logs.
* **Manage** (gear icon) â€” Integration Runtimes, Git configuration, Linked Services management, Triggers.

**Open Author:** click the **Author** icon (pencil) to build pipelines.

---

## ğŸ¯ 4. Authoring â€” Create Linked Service (Storage connection)

1. â­ Author view â†’ top-left area shows **Factory Resources** (Pipelines, Datasets, Data flows, etc.).
2. â­ Click the **Manage** icon (gear) on the left to open Manage view.
3. â­ Under Manage â†’ **Linked services** â†’ click **+ New**.
4. â­ Select **Azure Blob Storage** or **Azure Data Lake Storage Gen2**.
5. â­ Choose authentication (for learning: **Account key**) â†’ select the Storage Account from the dropdown or paste details â†’ click **Test connection** â†’ **Create**.

ğŸ“• **Note:** Linked Service stores connection and auth; datasets reference a linked service.

---

## ğŸ¯ 5. Authoring â€” Create a Dataset (Source / Sink)

1. Click **Author** (pencil) â†’ under **Factory Resources** expand **Datasets** â†’ click **+** â†’ **New dataset**.
2. Choose **Azure Blob Storage** (or ADLS Gen2) â†’ then choose **DelimitedText** (CSV).
3. On dataset configuration:

   * **Linked service:** choose the Linked Service you created (e.g., `AzureBlobStorage1`).
   * **Container:** click the folder icon to **browse** containers and select the file or folder.
   * **Directory / File name:** pick the exact file (for source) or leave File name blank for sink.
   * **First row as header:** toggle on if your CSV has header row.
   * **Import schema:** keep None for now (or import if you want typed schema).
4. Click **OK / Create**.

ğŸ’¡ **Tip:** Use descriptive dataset names like `ds_students_raw` and `ds_students_processed`.

---

## ğŸ¯ 6. Authoring â€” Create a Pipeline

1. Author â†’ **Pipelines** â†’ **+** â†’ **Pipeline**.
2. Rename the pipeline (click the title on top-left of canvas) e.g. `PL_CopyStudents`.
3. Left Activities pane â†’ expand **Move & transform** â†’ drag **Copy data** onto canvas.

ğŸ”§ **Configure Copy Activity:**

* Click the activity â†’ bottom pane shows tabs.
* â­ **Source tab:** choose your source dataset (`ds_students_raw`).
* â­ **Sink tab:** choose your sink dataset (`ds_students_processed`).
* â­ **Mapping:** click **Import schema** if you want ADF to auto-map columns.

ğŸ‰ **Save changes:** use **Publish all** on the top bar (big blue button) â€” this deploys your edits.

---

## ğŸ¯ 7. Debug, Publish, and Trigger 

* â­ **Debug:** use the **Debug** button (top toolbar inside the author canvas) to test-run without publishing.ğŸš€
* â­ After Debug success â†’ click **Publish all** (big blue button at top) to persist changes.ğŸš€
* â­  To run the pipeline manually after publish: click **Add trigger** â†’ **Trigger now**.
* â­  To schedule: **Add trigger** â†’ **New/Edit** â†’ set a schedule or event trigger.

âš¡ **Monitor runs:** Click the **Monitor** icon (left) â†’ click **Pipeline runs** â†’ click a run to see activity details and logs.

---

## ğŸ¯ 8. Verify Output in Storage

1. Return to the **Azure Portal** tab â†’ Storage account â†’ **Containers** â†’ open the sink container (e.g., `outputcontainer`).
2. Confirm a new file appears (ADF creates the file in the sink container).
3. If the sink dataset left File name blank, ADF may generate a filename (sometimes `.txt`).

---

## âœ… 9. Quick UI Checklist (Step-by-step) â€” ğŸ§­ Day 1 Flow

1. Azure Portal â†’ Resource groups â†’ select RG
2. RG â†’ click Storage Account â†’ Data storage â†’ Containers â†’ upload `student.csv` to `pracccontainer`
3. RG â†’ click Data Factory resource â†’ **Author & Monitor** â†’ opens Studio
4. Studio â†’ Manage â†’ Linked services â†’ + New â†’ create `AzureBlobStorage1` using Account Key
5. Studio â†’ Author â†’ Datasets â†’ + New â†’ DelimitedText â†’ create `ds_students_raw` (point to `pracccontainer/student.csv`)
6. Studio â†’ Author â†’ Datasets â†’ + New â†’ DelimitedText â†’ create `ds_students_processed` (point to `outputcontainer`, file blank)
7. Studio â†’ Author â†’ Pipelines â†’ + New Pipeline â†’ drag **Copy Data** activity
8. Configure Copy Data: Source = `ds_students_raw`, Sink = `ds_students_processed`
9. Click **Debug** â†’ confirm success â†’ Click **Publish all** â†’ Click **Add trigger â†’ Trigger now**
10. Portal â†’ Storage Account â†’ Containers â†’ `outputcontainer` â†’ verify copied file

---

## ğŸ§ª 10. Troubleshooting (UI-specific)

* **Can't find Containers**: Expand **Data storage** or search within the storage account left menu.
* **Linked service test fails**: re-check storage account name, keys, or network/firewall settings.
* **ADF shows 0 pipelines**: You must **Publish all** to persist authored objects; use Debug for quick testing.
* **Copy fails**: Open Monitor â†’ click the failed run â†’ read activity error message â†’ check dataset file path and permissions.

---

## ğŸ’¡ğŸ“• 11. Small Tips & Best Practices (UI habits)

* Use consistent naming: prefixes like `ds_`, `ls_`, `pl_` help navigation.
* Keep containers organized: `raw/`, `staging/`, `processed/`.
* Always use **Debug** while developing, then **Publish** when stable.
* Use the folder browser (folder icon) when creating datasets â€” it prevents path mistakes.

---

# ğŸ”¥ Day 2 â€” ADF: Step-by-step Azure Portal & ADF Studio Navigation

## âš¡ Table of contents

1. Day 2 â€” Ingest & Orchestrate (Portal â†’ ADF Studio)

   * 1.1 Upload practice CSV to Storage
   * 1.2 Create Linked Service in ADF
   * 1.3 Create Source Dataset
   * 1.4 Create Pipeline and Parameters
   * 1.5 Add Get Metadata
   * 1.6 Add If Condition
   * 1.7 Create Sink Dataset
   * 1.8 Add Copy Activity
   * 1.9 Add Pipeline Variables & Set Variable
   * 1.10 Debug, Monitor, Publish
2. Key Vault â€” Step-by-step (Portal â†’ Configure â†’ ADF)

   * 2.1 Create Key Vault
   * 2.2 Add a Secret
   * 2.3 Fix RBAC if needed (assign roles)
   * 2.4 Grant ADF access (RBAC)
   * 2.5 Create Key Vault Linked Service in ADF
   * 2.6 (Optional) Use Key Vault secret in Storage LS

---

# ğŸ’¥ 1. Day 2 â€” Ingest & Orchestrate (Portal â†’ ADF Studio)

> All steps assume you are signed-in to the Azure Portal with the subscription that contains your resources.

## ğŸ¯ 1.1 Upload practice CSV to Storage (Azure Portal)

1. Portal: click **All resources** â†’ open **Storage account** `practice2sa`.
2. In the storage blade (left menu) click **Containers** (under Data storage).
3. Click container name **praccontainer**.
4. Click **Upload** (top menu).

   * Files: choose local file `studentpracticecsvadls.csv`.
   * Destination path: leave blank (file at container root) or type `input/` to upload into a folder.
   * Click **Upload**.
5. Verify: container view shows `studentpracticecsvadls.csv`. Click the file to view properties and size.

> ğŸ’¡ Tip: If you want an `output` folder, create it now from container â†’ + Folder â†’ name `output`.

---

## ğŸ¯ 1.2 Create Linked Service in ADF (ADF Studio)

1. Portal: open your **Data Factory** resource â†’ click **Open Azure Data Factory Studio**.
2. In ADF Studio left rail click **Manage** (gear icon).
3. Under **Connections** click **Linked services**.
4. Click **+ New** â†’ choose **Azure Data Lake Storage Gen2** (or **Azure Blob Storage** depending on your account type) â†’ **Continue**.
5. Configure:

   * **Name**: `LS_practice2sa`
   * **Authentication**: choose **Account key** (or Managed Identity if you prefer) â€” for the account key, paste the key from Storage Account â†’ Access keys.
   * Click **Test connection** â†’ **Create**.

---

## ğŸ¯ 1.3 Create Source Dataset (ADF Studio)

1. In ADF Studio left rail click **Author** (pencil icon).
2. Under **Factory Resources** â†’ **Datasets** â†’ click **+** â†’ **Dataset**.
3. Choose **Azure Data Lake Storage Gen2** â†’ Format **DelimitedText** â†’ **Continue**.
4. **Name**: `ds_studentpractice` â†’ Click **OK**.
5. â­ In **Connection**:

   * Linked service: `LS_practice2sa`
   * Container: `praccontainer`
   * Directory: leave empty (root) or set `input` if you uploaded under `input/`
   * File: `studentpracticecsvadls.csv`
   * First row as header: **On**
   * Import schema: None (OK for now)
6. Click **Preview data** to confirm rows load.

---

## ğŸ¯ 1.4 Create Pipeline and Parameters

1. In **Author** â†’ under **Pipelines** click **+** â†’ **Pipeline**.
2. Rename pipeline to `PL_DynamicCopy`.
3. â­ On the pipeline properties pane click **Parameters** â†’ **+ New** three times and create:

   * `SourceFolder` (String)
   * `SourceFile` (String)
   * `TargetFile` (String)

âš¡ Expected test values later:

* â­ SourceFolder: (blank)
* â­ SourceFile: `studentpracticecsvadls.csv`
* â­ TargetFile: `output_student.csv`

---

## ğŸ¯ 1.5 Add Get Metadata Activity

1. From the Activities toolbox expand **General** â†’ drag **Get Metadata** onto the canvas.
2. Rename to `GetMeta_CheckFile` (click activity â†’ General tab â†’ rename).
3. â­ Settings tab:

   * Dataset: `ds_studentpractice` (no dataset params if dataset pointed to the file directly)
   * Field list: add `exists`, `size`, `lastModified`.

---

## ğŸ¯ 1.6 Add If Condition

1. From toolbox â†’ **Iteration & conditionals** â†’ drag **If Condition** to the canvas to the right of the Get Metadata.
2. Connect `GetMeta_CheckFile` â†’ drag green arrow to the `If Condition` (creates dependency).
3. Select the If Condition activity and rename to `If_FileExists`.
4. â­ Settings â†’ Expression â†’ Add dynamic content:

```
@activity('GetMeta_CheckFile').output.exists
```

---

## ğŸ¯ 1.7 Create Sink Dataset (parameterized)

1. Author â†’ Datasets â†’ + â†’ Azure Data Lake Storage Gen2 â†’ DelimitedText â†’ Continue.
2. Name: `ds_student_sink` â†’ OK.
3. Parameters tab: add parameter `TargetFile` (String).
4. â­ Connection tab:

   * Linked service: `LS_practice2sa`
   * Container: `praccontainer`
   * Directory/Folder: `output` (or root)
   * â­ File: Add dynamic content: `@dataset().TargetFile`
   * First row as header: On

---

## ğŸ¯ 1.8 Add Copy Data Activity (inside If True)

1. Select `If_FileExists` activity â†’ click the small pencil/edit icon to open the inner canvas.
2. From toolbox â†’ **Move & transform** â†’ drag **Copy Data** into the True canvas.
3. Rename to `Copy_To_Sink`.
4. Source tab: Dataset â†’ `ds_studentpractice`.
5. â­ Sink tab: Dataset â†’ `ds_student_sink` and set its dataset parameter `TargetFile` to dynamic: `@pipeline().parameters.TargetFile`.
6. (Optional) Mapping tab â†’ Auto-mapping.


---

## ğŸ¯ 1.9 Pipeline Variables & Set Variable

1. â­ Click the empty pipeline canvas â†’ on the right panel click **Variables** â†’ Add:

   * `FileSize` (String)
   * `Status` (String)
2. â­ Edit inner canvas (If True) â†’ from toolbox add **Set Variable** activity after `Copy_To_Sink`.

   * Rename to `Log_FileSize`.
   * Settings: Variable name `FileSize`, Value: dynamic content:

```
@string(activity('GetMeta_CheckFile').output.size)
```

3. â­ Switch to the False tab and add **Set Variable** activity named `Set_Status_FileMissing`.

   * Settings: Variable `Status`, Value: `"File Missing"`.

---

##  ğŸ¯ 1.10 Debug, Monitor & Publish

1. â¡ï¸ Click **Debug** â†’ enter parameters:

   * SourceFolder: (blank)
   * SourceFile: `studentpracticecsvadls.csv`
   * TargetFile: `output_student.csv`
2. â¡ï¸ Click **OK** and wait for the run.
3. â¡ï¸ Monitor: left rail â†’ **Monitor** â†’ open the latest pipeline run â†’ click activities to view **Input/Output** JSON.

   * Check `GetMeta_CheckFile` output: `exists: true`, `size: <number>`.
   * Check `Copy_To_Sink` output for `rowsRead` / `rowsCopied`.
4. Verify sink file exists: Storage Account `practice2sa` â†’ Container `praccontainer` â†’ Folder `output` â†’ `output_student.csv`.âš¡
5. âœ… If success â†’ Author pane â†’ top-right **Publish All**.

---

# ğŸ” 2. Key Vault â€” Step-by-step (Portal â†’ Configure â†’ ADF)

> This covers creating Key Vault, adding secret, fixing RBAC and wiring Key Vault within ADF.

## ğŸ¯ 2.1 Create Key Vault 

1. â­ Azure Portal â†’ **Create a resource** â†’ search **Key Vault** â†’ **Create**.
2. â­ Fill the form:

   * Subscription: Azure for Students
   * Resource group: `RGnew` (or your RG)
   * Key vault name: `storage-2-key` (or your chosen name)
   * Region: choose region (East US is fine)
   * Pricing tier: Standard
   * Soft-delete: Enabled is OK
3. â­ Click **Review + create** â†’ **Create**.
4. â­ After deployment click **Go to resource**.

## ğŸ¯ 2.2 Add a Secret ğŸ›¡ï¸

1. In Key Vault left menu â†’ click **Secrets**.
2. Click **+ Generate/Import**.
3. Name: `storage-key`.
4. â­ Value: paste the **Storage account Key** (from Storage Account â†’ Access keys â†’ Key1).
5. Click **Create**.

> ğŸ”§ If you see an RBAC error when adding a secret, proceed to fix RBAC (next section).

## ğŸ¯ 2.3 Fix RBAC (if needed)ğŸ”§

1. â­ In Key Vault â†’ left menu â†’ **Access control (IAM)**.
2. â­ Click **+ Add** â†’ **Add role assignment**.
3. â­ Choose a role for your user like **Key Vault Administrator** or **Key Vault Secrets Officer** and assign it to your user account.
4. â­ Click **Review + Assign**.
5. Wait 30â€“60 seconds and retry adding the secret.

## ğŸ¯ 2.4 Grant ADF access to Key Vault (RBAC mode)ğŸ‘¤ ğŸ†”

> ğŸ› ï¸ If your Key Vault uses RBAC for access management, do this:

1. â­Key Vault â†’ **Access control (IAM)** â†’ **+ Add** â†’ **Add role assignment**.
2. â­ Search for role **Key Vault Secrets User** (or **Key Vault Reader** if not available).
3. â­ Members â†’ **Select members** â†’ search for your Data Factory name (it will appear as a managed identity).
4. â­ Select it â†’ **Review + Assign**.

> ğŸ› ï¸ If the Key Vault uses legacy Access Policies (older UI), you'd use Key Vault â†’ Access policies â†’ Add policy â†’ select ADF managed identity and give **Get** and **List**.

## ğŸ¯ 2.5 Create Key Vault Linked Service in ADF âš™ï¸

1. ADF Studio â†’ **Manage** â†’ **Linked services** â†’ **+ New**.
2. Choose **Azure Key Vault** â†’ Continue.
3. Name: `LS_KeyVault`.
4. Base URL: choose the Key Vault resource `https://storage-2-key.vault.azure.net/`.
5. Authentication method: **Managed Identity** (System-assigned) â†’ Test connection â†’ Create.

## ğŸ¯ 2.6 (Optional) Use Key Vault secret in Storage Linked Service

Two common approaches:

ğŸ’¡ **A. If you want to use Key Vault secret (Account Key) in Storage LS** ğŸ› ï¸

1. Storage Account must allow shared key access.
2. In ADF Manage â†’ Linked services â†’ edit `LS_practice2sa`.
3. Change Authentication to **Account key (from Azure Key Vault)**.
4. Select `LS_KeyVault` and Secret name `storage-key`.
5. Test connection â†’ Save.

ğŸ’¡ **B. Recommended modern approach â€” Keep using Managed Identity for Storage** ğŸ› ï¸

1. Keep `LS_practice2sa` using Managed Identity authentication.
2. Ensure ADF managed identity has **Storage Blob Data Contributor** (or appropriate) role on the Storage Account via Storage Account â†’ Access control (IAM) â†’ Add role assignment.

---

# Quick checklist (copy this into your notes)âœ…

* [ ] studentpracticecsvadls.csv uploaded to `praccontainer`
* [ ] `LS_practice2sa` created and tested
* [ ] `ds_studentpractice` created and previewed
* [ ] `PL_DynamicCopy` with params created
* [ ] `GetMeta_CheckFile` added and configured
* [ ] `If_FileExists` added with correct expression
* [ ] `ds_student_sink` created with parameterized file name
* [ ] `Copy_To_Sink` added and linked to pipeline params
* [ ] Pipeline variables `FileSize`, `Status` added and logged
* [ ] Debugged pipeline and verified sink file
* [ ] Key Vault `storage-2-key` created
* [ ] Secret `storage-key` added to Key Vault
* [ ] RBAC configured so ADF can read secrets
* [ ] `LS_KeyVault` linked service created
* [ ] Storage linked service updated (if needed)

---

# ğŸ“• Notes

* â­Use Managed Identity for production â€” it is modern and secure.
* â­ Key Vault is primarily for secrets you don't want in plain text (SQL passwords, API keys, storage keys if you prefer key-based auth).
* â­ Save and publish often. Keep a Git branch for your ADF factory if you plan CI/CD later.

---

*End of navigation cheatsheet â€” keep this as a pinned quick reference while you practice in ADF Studio.*

